{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for csv_file in ['amazon_cells_labelled.txt' , 'imdb_labelled.txt' , 'yelp_labelled.txt']:\n",
    "    temp_df = pd.read_csv(csv_file , sep=\"\\t\" , header = 0 , names = ['text' , 'sentiment'])\n",
    "    df_list.append(temp_df)\n",
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have to jiggle the plug to get it to line up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2745 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  sentiment\n",
       "0                          Good case, Excellent value.          1\n",
       "1                               Great for the jawbone.          1\n",
       "2    Tied to charger for conversations lasting more...          0\n",
       "3                                    The mic is great.          1\n",
       "4    I have to jiggle the plug to get it to line up...          0\n",
       "..                                                 ...        ...\n",
       "994  I think food should have flavor and texture an...          0\n",
       "995                           Appetite instantly gone.          0\n",
       "996  Overall I was not impressed and would not go b...          0\n",
       "997  The whole experience was underwhelming, and I ...          0\n",
       "998  Then, as if I hadn't wasted enough of my life ...          0\n",
       "\n",
       "[2745 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1385\n",
       "0    1360\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the ditribution of classes to get an idea before implementing any classification task\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>Plus, I seriously do not believe it is worth i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Light weight, I hardly notice it is there.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>This movie is so awesome!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>Editing: The editing of this film was phenomen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>If you like a loud buzzing to override all you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Anyway, this FS restaurant has a wonderful bre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>Don't bother coming here.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  sentiment\n",
       "622  Plus, I seriously do not believe it is worth i...          0\n",
       "194         Light weight, I hardly notice it is there.          1\n",
       "585                        This movie is so awesome!            1\n",
       "608  Editing: The editing of this film was phenomen...          1\n",
       "178  If you like a loud buzzing to override all you...          0\n",
       "441  Anyway, this FS restaurant has a wonderful bre...          1\n",
       "936                          Don't bother coming here.          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at any random 7 texts from the data and the sentiment assigned to them\n",
    "df[['text' , 'sentiment']].sample(7 , random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df , test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['sentiment']\n",
    "y_test = df_test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(ngram_range=(1,3), min_df=3, strip_accents='ascii')\n",
    "\n",
    "x_train = vec.fit_transform(df_train['text'])\n",
    "x_test = vec.transform(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(fit_prior=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "\n",
    "p, r, f, s = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "\n",
    "pd1 = pd.DataFrame(\n",
    "    {\n",
    "        'Precision': p,\n",
    "        'Recall': r,\n",
    "        'F': f,\n",
    "        'Support': s,\n",
    "    },\n",
    "    index=[0,1] \n",
    ").round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB(fit_prior=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "\n",
    "p, r, f, s = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "\n",
    "pd2 = pd.DataFrame(\n",
    "    {\n",
    "        'Precision': p,\n",
    "        'Recall': r,\n",
    "        'F': f,\n",
    "        'Support': s,\n",
    "    },\n",
    "    index=[0,1] \n",
    ").round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "\n",
    "p, r, f, s = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "\n",
    "pd3 = pd.DataFrame(\n",
    "    {\n",
    "        'Precision': p,\n",
    "        'Recall': r,\n",
    "        'F': f,\n",
    "        'Support': s,\n",
    "    },\n",
    "    index=[0,1] \n",
    ").round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Precision  Recall       F  Support\n",
      "0     0.8134  0.8044  0.8089      271\n",
      "1     0.8114  0.8201  0.8157      278\n",
      "\n",
      "\n",
      "   Precision  Recall       F  Support\n",
      "0     0.8040  0.7417  0.7716      271\n",
      "1     0.7659  0.8237  0.7938      278\n",
      "\n",
      "\n",
      "   Precision  Recall       F  Support\n",
      "0     0.8175  0.8266  0.8220      271\n",
      "1     0.8291  0.8201  0.8246      278\n"
     ]
    }
   ],
   "source": [
    "#Comparing the scores for the different models that have been implemented- Multinomial Naive Bayes, Bernoulli Naive\n",
    "#Bayes and the Logistic Regression. \n",
    "print(pd1)\n",
    "print(\"\\n\")\n",
    "print(pd2)\n",
    "print(\"\\n\")\n",
    "print(pd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since I am going to implement Grid Search later, which only takes one parameter, a pipeline wrapper is used to combine\n",
    "#multiple estimators into one\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipe = Pipeline(steps = [('CountVectorizer' , CountVectorizer()) , ('MultinomialNB' , MultinomialNB()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CountVectorizer__ngram_range': (1, 2), 'MultinomialNB__alpha': 1, 'MultinomialNB__fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "#The Grid Search technique is used to find the optimal hyperparametrs for the estimators. This helps to improve the\n",
    "#score of our model\n",
    "param_grid = {'CountVectorizer__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "             'MultinomialNB__alpha': [0.1,1],\n",
    "             'MultinomialNB__fit_prior': [True, False],\n",
    "             }\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "search = GridSearchCV(pipe,param_grid,scoring='precision_macro',n_jobs=-1)\n",
    "search.fit(df_train['text'],y_train)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision = 84.89% & Recall=84.89% & F-score=84.88%  \n"
     ]
    }
   ],
   "source": [
    "y_test_pred = search.predict(df_test['text'])\n",
    "\n",
    "p, r, f, s = precision_recall_fscore_support(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(f'Macro Precision = {p:.2%} & Recall={r:.2%} & F-score={f:.2%}  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Therefore we can see that after implementing Grid Search to optimize our hyperparameters, our various scores have risen\n",
    "#We also implemented various models, but could not find any significant difference in their scores.\n",
    "#Finally we can conclude that, we can classify sentences, coming from three different documents, with a precision close\n",
    "#to 85%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
